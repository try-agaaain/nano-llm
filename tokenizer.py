"""
TinyStories è‡ªå®šä¹‰åˆ†è¯å™¨ - åŸºäº BPE ç®—æ³•ï¼Œé’ˆå¯¹ TinyStories æ•°æ®é›†ä¼˜åŒ–
æ”¯æŒä» CSV å’Œ HuggingFace æ•°æ®é›†åŠ è½½
"""

import os
import tempfile
import csv
from pathlib import Path
from typing import List, Optional, Iterator, Union

from transformers import PreTrainedTokenizerFast
from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders
from datasets import load_dataset
from tqdm import tqdm


class TinyStoriesTokenizerFast(PreTrainedTokenizerFast):
    """
    TinyStories åˆ†è¯å™¨ - ç»§æ‰¿ PreTrainedTokenizerFast
    é’ˆå¯¹è‹±æ–‡ç«¥è¯æ•…äº‹æ–‡æœ¬ä¼˜åŒ–çš„ BPE åˆ†è¯å™¨
    """
    
    # å‘Šè¯‰çˆ¶ç±»ï¼Œåº•å±‚ tokenizers åº“çš„æ–‡ä»¶å«ä»€ä¹ˆåå­—
    tokenizer_file = "tokenizer.json"
    
    # æ¨¡å‹è¾“å…¥åç§°
    model_input_names = ["input_ids", "attention_mask"]
    
    def __init__(
        self,
        tokenizer_object: Optional[Tokenizer] = None,
        unk_token="<unk>",
        pad_token="<pad>",
        bos_token="<bos>",
        eos_token="<eos>",
        **kwargs
    ):
        """
        åˆå§‹åŒ–æ–¹æ³•é…ç½®ç‰¹æ®Šæ ‡è®°å¹¶è°ƒç”¨çˆ¶ç±»çš„ __init__ã€‚
        
        Args:
            tokenizer_object: åº•å±‚çš„ tokenizers.Tokenizer å¯¹è±¡
            unk_token: æœªçŸ¥æ ‡è®°ï¼Œé»˜è®¤ "<unk>"
            pad_token: å¡«å……æ ‡è®°ï¼Œé»˜è®¤ "<pad>"
            bos_token: å¼€å§‹æ ‡è®°ï¼Œé»˜è®¤ "<bos>"
            eos_token: ç»“æŸæ ‡è®°ï¼Œé»˜è®¤ "<eos>"
            **kwargs: å…¶ä»–å‚æ•°ä¼ é€’ç»™çˆ¶ç±»
        """
        super().__init__(
            tokenizer_object=tokenizer_object,
            unk_token=unk_token,
            pad_token=pad_token,
            bos_token=bos_token,
            eos_token=eos_token,
            **kwargs,
        )
    
    @classmethod
    def from_pretrained(cls, model_id_or_path: str, **kwargs) -> "TinyStoriesTokenizerFast":
        """
        ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½ tokenizer
        
        Args:
            model_id_or_path: æ¨¡å‹ ID æˆ–æœ¬åœ°è·¯å¾„
            **kwargs: å…¶ä»–å‚æ•°ä¼ é€’ç»™çˆ¶ç±»
        
        Returns:
            TinyStoriesTokenizerFast: åŠ è½½çš„ tokenizer å®ä¾‹
        """
        tokenizer = super().from_pretrained(model_id_or_path, **kwargs)
        print(f"âœ… å·²åŠ è½½ TinyStories åˆ†è¯å™¨ (è¯è¡¨å¤§å°: {tokenizer.vocab_size})")
        return tokenizer


def train_tokenizer_from_tinystories(
    save_path: str,
    vocab_size: int = 8192,
    num_samples: int = 50000,
    split: str = "train",
    dataset_dir: Optional[str] = None,
    csv_path: Optional[str] = None,
    text_column: str = "text",
) -> TinyStoriesTokenizerFast:
    """
    ä» TinyStories æ•°æ®é›†æˆ– CSV æ–‡ä»¶è®­ç»ƒ BPE åˆ†è¯å™¨
    
    Args:
        save_path: ä¿å­˜è·¯å¾„
        vocab_size: è¯è¡¨å¤§å°ï¼ˆé»˜è®¤ 8192ï¼Œæ¯” GPT-2 çš„ 50k å°å¾ˆå¤šï¼‰
        num_samples: ç”¨äºè®­ç»ƒåˆ†è¯å™¨çš„æ ·æœ¬æ•°é‡ï¼ˆé»˜è®¤ 50000ï¼‰
        split: æ•°æ®é›†åˆ†å‰²ï¼ˆtrain/validationï¼‰
        dataset_dir: æ•°æ®é›†ç¼“å­˜ç›®å½•ï¼ˆå¦‚æœä¸º Noneï¼Œä½¿ç”¨é»˜è®¤çš„ dataset ç›®å½•ï¼‰
        csv_path: CSV æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœæä¾›ï¼Œä¼˜å…ˆäºæ•°æ®é›†ï¼‰
        text_column: CSV ä¸­æ–‡æœ¬åˆ—çš„åç§°ï¼ˆé»˜è®¤ "text"ï¼‰
    
    Returns:
        TinyStoriesTokenizerFast: è®­ç»ƒåçš„ tokenizer
    """
    print(f"ğŸ“š ä» TinyStories æ•°æ®é›†è®­ç»ƒåˆ†è¯å™¨...")
    print(f"   è¯è¡¨å¤§å°: {vocab_size}")
    print(f"   è®­ç»ƒæ ·æœ¬æ•°: {num_samples}")
    
    # 1. åˆå§‹åŒ– BPE Tokenizer
    tokenizer = Tokenizer(models.BPE(unk_token="<unk>"))
    # ä½¿ç”¨ Whitespace pre-tokenizer æ›¿ä»£ ByteLevelï¼Œé¿å…å­—ç¬¦çº§åˆ†å‰²
    # è¿™æ ·å¯ä»¥è®© BPE å­¦ä¹ åˆ°çœŸæ­£çš„å­è¯å•å…ƒ
    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()
    
    # ç‰¹æ®Šæ ‡è®°ï¼ˆé’ˆå¯¹è¯­è¨€æ¨¡å‹ï¼‰
    special_tokens = ["<unk>", "<pad>", "<bos>", "<eos>"]
    
    # 2. é…ç½® BPE è®­ç»ƒå™¨
    trainer = trainers.BpeTrainer(
        vocab_size=vocab_size,
        special_tokens=special_tokens,
        show_progress=True,
        min_frequency=2,  # æœ€å°å‡ºç°é¢‘ç‡
    )
    
    # 3. æ”¶é›†è®­ç»ƒæ–‡æœ¬
    print("   æ­£åœ¨æ”¶é›†è®­ç»ƒæ–‡æœ¬...")
    
    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶æ¥å­˜å‚¨è®­ç»ƒæ–‡æœ¬
    temp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8')
    temp_file_path = temp_file.name
    
    try:
        collected_samples = 0
        
        # ä» CSV æˆ–æ•°æ®é›†åŠ è½½æ•°æ®
        if csv_path and os.path.exists(csv_path):
            print(f"   ä» CSV æ–‡ä»¶åŠ è½½: {csv_path}")
            with open(csv_path, 'r', encoding='utf-8') as f:
                csv_reader = csv.DictReader(f)
                for row in tqdm(csv_reader, desc="æ”¶é›†æ–‡æœ¬", total=num_samples):
                    if collected_samples >= num_samples:
                        break
                    
                    # è·å–æ–‡æœ¬å­—æ®µ
                    if text_column not in row:
                        # å°è¯•å¯»æ‰¾å…¶ä»–å¯èƒ½çš„åˆ—å
                        for col in ["text", "story", "content", "narrative"]:
                            if col in row:
                                text = row[col]
                                break
                        else:
                            # ä½¿ç”¨ç¬¬ä¸€ä¸ªéç©ºå­—æ®µ
                            text = next((v for v in row.values() if v), None)
                    else:
                        text = row[text_column]
                    
                    if not text or not isinstance(text, str) or not text.strip():
                        continue
                    
                    # æ¸…ç†æ–‡æœ¬
                    cleaned_text = "\n".join(line.strip() for line in text.split("\n") if line.strip())
                    temp_file.write(cleaned_text + "\n\n")
                    collected_samples += 1
        else:
            # ä» HuggingFace æ•°æ®é›†åŠ è½½
            print(f"   ä» HuggingFace æ•°æ®é›†åŠ è½½ ({split} åˆ†å‰²)...")
            
            # è®¾ç½®æ•°æ®é›†ç¼“å­˜ç›®å½•
            if dataset_dir is None:
                # é»˜è®¤ä½¿ç”¨å½“å‰ç›®å½•ä¸‹çš„ dataset æ–‡ä»¶å¤¹
                dataset_dir = str(Path.cwd() / "dataset")
            
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            Path(dataset_dir).mkdir(parents=True, exist_ok=True)
            
            # ä½¿ç”¨éæµå¼åŠ è½½ï¼Œç¡®ä¿æ•°æ®çœŸå®ä¸‹è½½åˆ° cache_dir
            try:
                dataset = load_dataset(
                    "./dataset",
                    split=split,
                    streaming=False,
                    cache_dir=dataset_dir,
                    download_mode="force_redownload",
                )
                
                for example in tqdm(dataset, desc="æ”¶é›†æ–‡æœ¬", total=num_samples):
                    if collected_samples >= num_samples:
                        break
                    
                    # å°è¯•å¤šç§å¯èƒ½çš„å­—æ®µå
                    story = None
                    for field_name in ["text", "story", "content", "narrative", "sentence"]:
                        if field_name in example:
                            story = example[field_name]
                            break
                    
                    if story is None:
                        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå­—ç¬¦ä¸²å­—æ®µ
                        for key, value in example.items():
                            if isinstance(value, str):
                                story = value
                                break
                    
                    if story is None or not isinstance(story, str) or not story.strip():
                        continue
                    
                    # æ¸…ç†æ–‡æœ¬
                    cleaned_story = "\n".join(line.strip() for line in story.split("\n") if line.strip())
                    temp_file.write(cleaned_story + "\n\n")
                    collected_samples += 1
            except Exception as e:
                print(f"   âš ï¸  ä» HuggingFace åŠ è½½å¤±è´¥: {e}")
                print(f"   è¯·ç¡®ä¿æä¾›äº† CSV æ–‡ä»¶è·¯å¾„æˆ–æ•°æ®é›†ç›®å½•")
                raise
        
        temp_file.close()
        print(f"   âœ… å·²æ”¶é›† {collected_samples} ä¸ªæ ·æœ¬")
        
        # è®­ç»ƒåˆ†è¯å™¨
        print("   æ­£åœ¨è®­ç»ƒ BPE åˆ†è¯å™¨...")
        tokenizer.train(
            files=[temp_file_path],
            trainer=trainer
        )
        
        print(f"   âœ… è®­ç»ƒå®Œæˆ (è¯è¡¨å¤§å°: {tokenizer.get_vocab_size()})")
        
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(temp_file_path):
            os.unlink(temp_file_path)
    
    # 5. è®¾ç½®è§£ç å™¨ - ä½¿ç”¨ BPEDecoder
    # ç”±äºä½¿ç”¨ Whitespace pre-tokenizerï¼ŒBPEDecoder å°±è¶³å¤Ÿäº†
    tokenizer.decoder = decoders.BPEDecoder()
    
    # 6. ä¿å­˜åº•å±‚æ–‡ä»¶ (tokenizer.json)
    if save_path:
        Path(save_path).mkdir(parents=True, exist_ok=True)
        tokenizer.save(str(Path(save_path) / "tokenizer.json"), pretty=True)
        print(f"   ğŸ’¾ å·²ä¿å­˜åˆ°: {save_path}/tokenizer.json")
    
    # 7. åˆ›å»º TinyStoriesTokenizerFast å®ä¾‹
    fast_tokenizer = TinyStoriesTokenizerFast(tokenizer_object=tokenizer)
    
    # 8. ä¿å­˜é…ç½®æ–‡ä»¶ï¼ˆç”Ÿæˆ tokenizer_config.jsonï¼‰
    fast_tokenizer.save_pretrained(save_path)
    print(f"   âœ… åˆ†è¯å™¨å·²ä¿å­˜åˆ°: {save_path}")
    
    return fast_tokenizer


def load_or_train_tokenizer(
    tokenizer_path: Optional[str] = None,
    vocab_size: int = 8192,
    num_samples: int = 50000,
    force_retrain: bool = False,
    dataset_dir: Optional[str] = None,
    csv_path: Optional[str] = None,
    text_column: str = "text",
) -> TinyStoriesTokenizerFast:
    """
    åŠ è½½å·²å­˜åœ¨çš„åˆ†è¯å™¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è®­ç»ƒæ–°çš„
    
    Args:
        tokenizer_path: åˆ†è¯å™¨ä¿å­˜è·¯å¾„ï¼ˆå¦‚æœä¸º Noneï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„ï¼‰
        vocab_size: è¯è¡¨å¤§å°ï¼ˆä»…åœ¨è®­ç»ƒæ—¶ä½¿ç”¨ï¼‰
        num_samples: è®­ç»ƒæ ·æœ¬æ•°ï¼ˆä»…åœ¨è®­ç»ƒæ—¶ä½¿ç”¨ï¼‰
        force_retrain: æ˜¯å¦å¼ºåˆ¶é‡æ–°è®­ç»ƒ
        dataset_dir: æ•°æ®é›†ç¼“å­˜ç›®å½•ï¼ˆå¦‚æœä¸º Noneï¼Œä½¿ç”¨é»˜è®¤çš„ dataset ç›®å½•ï¼‰
        csv_path: CSV æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœæä¾›ï¼Œä¼˜å…ˆäºæ•°æ®é›†ï¼‰
        text_column: CSV ä¸­æ–‡æœ¬åˆ—çš„åç§°ï¼ˆé»˜è®¤ "text"ï¼‰
    
    Returns:
        TinyStoriesTokenizerFast: åˆ†è¯å™¨å®ä¾‹
    """
    if tokenizer_path is None:
        tokenizer_path = "./tokenizer"
    
    tokenizer_path = Path(tokenizer_path)
    tokenizer_json = tokenizer_path / "tokenizer.json"
    
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨å·²è®­ç»ƒçš„åˆ†è¯å™¨
    if not force_retrain and tokenizer_json.exists():
        print(f"ğŸ“– åŠ è½½å·²å­˜åœ¨çš„åˆ†è¯å™¨: {tokenizer_path}")
        return TinyStoriesTokenizerFast.from_pretrained(str(tokenizer_path))
    else:
        print(f"ğŸ”¨ è®­ç»ƒæ–°çš„åˆ†è¯å™¨...")
        return train_tokenizer_from_tinystories(
            save_path=str(tokenizer_path),
            vocab_size=vocab_size,
            num_samples=num_samples,
            dataset_dir=dataset_dir,
            csv_path=csv_path,
            text_column=text_column,
        )

if __name__ == "__main__":
    # è®¾ç½®æ•°æ®é›†ç›®å½•ï¼ˆé»˜è®¤ä½¿ç”¨å½“å‰ç›®å½•ä¸‹çš„ dataset æ–‡ä»¶å¤¹ï¼‰
    dataset_dir = str(Path.cwd() / "dataset")
    
    tokenizer = load_or_train_tokenizer(
        tokenizer_path="./tokenizer",
        vocab_size=8192,
        num_samples=10000,  # å…ˆç”¨è¾ƒå°æ ·æœ¬æ¼”ç¤ºä¿®å¤æ•ˆæœ
        force_retrain=True,  # å¼ºåˆ¶é‡æ–°è®­ç»ƒä»¥åº”ç”¨ä¿®å¤
        dataset_dir=dataset_dir,
    )
    print(f"âœ… åˆ†è¯å™¨è¯è¡¨å¤§å°: {tokenizer.vocab_size}")
    
    # æµ‹è¯•ç¼–è§£ç æ˜¯å¦æ­£å¸¸
    print("\n" + "="*70)
    print("ç¼–è§£ç æµ‹è¯•")
    print("="*70)
    
    test_texts = [
        "Hello world",
        "The little girl",
        "In the forest",
        "Once upon a time there was a beautiful day",
    ]
    
    for text in test_texts:
        print(f"\nåŸå§‹æ–‡æœ¬: {text}")
        
        # ç¼–ç 
        encoded = tokenizer.encode(text, return_tensors="pt")
        token_ids = encoded[0].tolist()
        print(f"Token IDs: {token_ids}")
        
        # è§£ç 
        decoded = tokenizer.decode(token_ids, skip_special_tokens=True)
        print(f"è§£ç æ–‡æœ¬: {decoded}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰Ä ç¬¦å·
        has_symbols = "Ä " in decoded or "ÄŠ" in decoded
        status = "âŒ æœ‰ä¹±ç ç¬¦å·" if has_symbols else "âœ… æ­£å¸¸"
        print(f"çŠ¶æ€: {status}")
    
    print("\n" + "="*70)