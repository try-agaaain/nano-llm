"""
TinyStories è‡ªå®šä¹‰åˆ†è¯å™¨ - åŸºäº BPE ç®—æ³•ï¼Œé’ˆå¯¹ TinyStories æ•°æ®é›†ä¼˜åŒ–
"""

import os
from pathlib import Path
from typing import Optional
import tempfile

from transformers import PreTrainedTokenizerFast
from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders
from src.dataset import TinyStoriesDataset


class TinyStoriesTokenizerFast(PreTrainedTokenizerFast):
    """
    TinyStories åˆ†è¯å™¨ - ç»§æ‰¿ PreTrainedTokenizerFast
    é’ˆå¯¹è‹±æ–‡ç«¥è¯æ•…äº‹æ–‡æœ¬ä¼˜åŒ–çš„ BPE åˆ†è¯å™¨
    """
    
    # å‘Šè¯‰çˆ¶ç±»ï¼Œåº•å±‚ tokenizers åº“çš„æ–‡ä»¶å«ä»€ä¹ˆåå­—
    tokenizer_file = "tokenizer.json"
    
    # æ¨¡å‹è¾“å…¥åç§°
    model_input_names = ["input_ids", "attention_mask"]
    
    def __init__(
        self,
        tokenizer_object: Optional[Tokenizer] = None,
        unk_token="<unk>",
        pad_token="<pad>",
        bos_token="<bos>",
        eos_token="<eos>",
        **kwargs
    ):
        """
        åˆå§‹åŒ–æ–¹æ³•é…ç½®ç‰¹æ®Šæ ‡è®°å¹¶è°ƒç”¨çˆ¶ç±»çš„ __init__ã€‚
        
        Args:
            tokenizer_object: åº•å±‚çš„ tokenizers.Tokenizer å¯¹è±¡
            unk_token: æœªçŸ¥æ ‡è®°ï¼Œé»˜è®¤ "<unk>"
            pad_token: å¡«å……æ ‡è®°ï¼Œé»˜è®¤ "<pad>"
            bos_token: å¼€å§‹æ ‡è®°ï¼Œé»˜è®¤ "<bos>"
            eos_token: ç»“æŸæ ‡è®°ï¼Œé»˜è®¤ "<eos>"
            **kwargs: å…¶ä»–å‚æ•°ä¼ é€’ç»™çˆ¶ç±»
        """
        super().__init__(
            tokenizer_object=tokenizer_object,
            unk_token=unk_token,
            pad_token=pad_token,
            bos_token=bos_token,
            eos_token=eos_token,
            **kwargs,
        )
    
    @classmethod
    def from_pretrained(cls, model_id_or_path: str, **kwargs) -> "TinyStoriesTokenizerFast":
        """
        ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½ tokenizer
        
        Args:
            model_id_or_path: æ¨¡å‹ ID æˆ–æœ¬åœ°è·¯å¾„
            **kwargs: å…¶ä»–å‚æ•°ä¼ é€’ç»™çˆ¶ç±»
        
        Returns:
            TinyStoriesTokenizerFast: åŠ è½½çš„ tokenizer å®ä¾‹
        """
        tokenizer = super().from_pretrained(model_id_or_path, **kwargs)
        print(f"âœ… å·²åŠ è½½ TinyStories åˆ†è¯å™¨ (è¯è¡¨å¤§å°: {tokenizer.vocab_size})")
        return tokenizer


def train_tokenizer_from_dataset(
    save_path: str,
    dataset,
    vocab_size: int = 8192,
    num_samples: int = 50000,
) -> TinyStoriesTokenizerFast:
    """
    ä»æ•°æ®é›†è®­ç»ƒ BPE åˆ†è¯å™¨
    
    Args:
        save_path: ä¿å­˜è·¯å¾„
        dataset: BaseDataset å®ä¾‹
        vocab_size: è¯è¡¨å¤§å°ï¼ˆé»˜è®¤ 8192ï¼‰
        num_samples: ç”¨äºè®­ç»ƒåˆ†è¯å™¨çš„æ ·æœ¬æ•°é‡ï¼ˆé»˜è®¤ 50000ï¼‰
    
    Returns:
        TinyStoriesTokenizerFast: è®­ç»ƒåçš„ tokenizer
    """
    print(f"ğŸ“š ä»æ•°æ®é›†è®­ç»ƒåˆ†è¯å™¨...")
    print(f"   è¯è¡¨å¤§å°: {vocab_size}")
    print(f"   è®­ç»ƒæ ·æœ¬æ•°: {num_samples}")
    
    # 1. åˆå§‹åŒ– BPE Tokenizer
    tokenizer = Tokenizer(models.BPE(unk_token="<unk>"))
    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()
    
    # ç‰¹æ®Šæ ‡è®°ï¼ˆé’ˆå¯¹è¯­è¨€æ¨¡å‹ï¼‰
    special_tokens = ["<unk>", "<pad>", "<bos>", "<eos>"]
    
    # 2. é…ç½® BPE è®­ç»ƒå™¨
    trainer = trainers.BpeTrainer(
        vocab_size=vocab_size,
        special_tokens=special_tokens,
        show_progress=True,
        min_frequency=2,
    )
    
    # 3. ä»æ•°æ®é›†è·å–æ–‡æœ¬å¹¶åˆ›å»ºä¸´æ—¶è®­ç»ƒæ–‡ä»¶
    print("   æ­£åœ¨å‡†å¤‡è®­ç»ƒæ•°æ®...")
    Path(save_path).mkdir(parents=True, exist_ok=True)
    
    with tempfile.NamedTemporaryFile(mode='w', encoding='utf-8', delete=False, suffix='.txt') as f:
        train_file_path = f.name
        texts = dataset.get_texts(num_samples=num_samples)
        for text in texts:
            f.write(text + "\n\n")
    
    try:
        # 4. è®­ç»ƒåˆ†è¯å™¨
        print("   æ­£åœ¨è®­ç»ƒ BPE åˆ†è¯å™¨...")
        tokenizer.train(
            files=[train_file_path],
            trainer=trainer
        )
        print(f"   âœ… è®­ç»ƒå®Œæˆ (è¯è¡¨å¤§å°: {tokenizer.get_vocab_size()})")
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(train_file_path):
            os.unlink(train_file_path)
    
    # 5. è®¾ç½®è§£ç å™¨
    tokenizer.decoder = decoders.BPEDecoder()
    
    # 6. ä¿å­˜åº•å±‚æ–‡ä»¶
    tokenizer.save(str(Path(save_path) / "tokenizer.json"), pretty=True)
    print(f"   ğŸ’¾ å·²ä¿å­˜åˆ°: {save_path}/tokenizer.json")
    
    # 7. åˆ›å»º TinyStoriesTokenizerFast å®ä¾‹å¹¶ä¿å­˜
    fast_tokenizer = TinyStoriesTokenizerFast(tokenizer_object=tokenizer)
    fast_tokenizer.save_pretrained(save_path)
    print(f"   âœ… åˆ†è¯å™¨å·²ä¿å­˜åˆ°: {save_path}")
    
    return fast_tokenizer


def load_or_train_tokenizer(
    tokenizer_path: Optional[str] = None,
    dataset=None,
    vocab_size: int = 8192,
    num_samples: int = 50000,
    force_retrain: bool = False,
) -> TinyStoriesTokenizerFast:
    """
    åŠ è½½å·²å­˜åœ¨çš„åˆ†è¯å™¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è®­ç»ƒæ–°çš„
    
    Args:
        tokenizer_path: åˆ†è¯å™¨ä¿å­˜è·¯å¾„ï¼ˆå¦‚æœä¸º Noneï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„ï¼‰
        dataset: BaseDataset å®ä¾‹ï¼ˆè®­ç»ƒæ—¶å¿…é¡»æä¾›ï¼‰
        vocab_size: è¯è¡¨å¤§å°ï¼ˆä»…åœ¨è®­ç»ƒæ—¶ä½¿ç”¨ï¼‰
        num_samples: è®­ç»ƒæ ·æœ¬æ•°ï¼ˆä»…åœ¨è®­ç»ƒæ—¶ä½¿ç”¨ï¼‰
        force_retrain: æ˜¯å¦å¼ºåˆ¶é‡æ–°è®­ç»ƒ
    
    Returns:
        TinyStoriesTokenizerFast: åˆ†è¯å™¨å®ä¾‹
    """
    if tokenizer_path is None:
        tokenizer_path = "./tokenizer"
    
    tokenizer_path = Path(tokenizer_path)
    tokenizer_json = tokenizer_path / "tokenizer.json"
    
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨å·²è®­ç»ƒçš„åˆ†è¯å™¨
    if not force_retrain and tokenizer_json.exists():
        print(f"ğŸ“– åŠ è½½å·²å­˜åœ¨çš„åˆ†è¯å™¨: {tokenizer_path}")
        return TinyStoriesTokenizerFast.from_pretrained(str(tokenizer_path))
    else:
        if dataset is None:
            raise ValueError("è®­ç»ƒåˆ†è¯å™¨æ—¶å¿…é¡»æä¾› dataset å‚æ•°")
        print(f"ğŸ”¨ è®­ç»ƒæ–°çš„åˆ†è¯å™¨...")
        return train_tokenizer_from_dataset(
            save_path=str(tokenizer_path),
            dataset=dataset,
            vocab_size=vocab_size,
            num_samples=num_samples,
        )

if __name__ == "__main__":
    # ç¤ºä¾‹ï¼šéœ€è¦æä¾›æ•°æ®é›†ç›®å½•
    from src.dataset import TinyStoriesDataset
    
    data_dir = "path/to/tinystories/dataset"  # è¯·æ›¿æ¢ä¸ºå®é™…è·¯å¾„
    
    try:
        dataset = TinyStoriesDataset(data_dir)
        
        tokenizer = load_or_train_tokenizer(
            tokenizer_path="./tokenizer",
            dataset=dataset,
            vocab_size=8192,
            num_samples=10000,
            force_retrain=True,
        )
        print(f"âœ… åˆ†è¯å™¨è¯è¡¨å¤§å°: {tokenizer.vocab_size}")
        
        # æµ‹è¯•ç¼–è§£ç æ˜¯å¦æ­£å¸¸
        print("\n" + "="*70)
        print("ç¼–è§£ç æµ‹è¯•")
        print("="*70)
        
        test_texts = [
            "Hello world",
            "The little girl",
            "In the forest",
            "Once upon a time there was a beautiful day",
        ]
        
        for text in test_texts:
            print(f"\nåŸå§‹æ–‡æœ¬: {text}")
            
            # ç¼–ç 
            encoded = tokenizer.encode(text, return_tensors="pt")
            token_ids = encoded[0].tolist()
            print(f"Token IDs: {token_ids}")
            
            # è§£ç 
            decoded = tokenizer.decode(token_ids, skip_special_tokens=True)
            print(f"è§£ç æ–‡æœ¬: {decoded}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰Ä ç¬¦å·
            has_symbols = "Ä " in decoded or "ÄŠ" in decoded
            status = "âŒ æœ‰ä¹±ç ç¬¦å·" if has_symbols else "âœ… æ­£å¸¸"
            print(f"çŠ¶æ€: {status}")
        
        print("\n" + "="*70)
    except ValueError as e:
        print(f"âŒ é”™è¯¯: {e}")
        print("   è¯·æä¾›æœ‰æ•ˆçš„æ•°æ®é›†ç›®å½•è·¯å¾„")