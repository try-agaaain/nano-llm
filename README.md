# NanoLLM - è½»é‡çº§å¤šå¤´æ³¨æ„åŠ›LLM

ä¸€ä¸ªä½¿ç”¨TinyStoriesæ•°æ®é›†å’ŒHuggingFaceåˆ†è¯å™¨çš„å®Œæ•´LLMå®ç°ã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…ä¾èµ–
```bash
pip install torch datasets transformers tokenizers tqdm
```

### è¿è¡Œè®­ç»ƒ
```bash
python train.py
```

### è¿è¡Œæµ‹è¯•
```bash
python test.py
```

### äº¤äº’èœå•
```bash
python quickstart.py
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
nano-llm/
â”œâ”€â”€ model.py       (580è¡Œ) - å¤šå¤´æ³¨æ„åŠ›LLMæ¨¡å‹
â”œâ”€â”€ train.py       (250è¡Œ) - TinyStoriesè®­ç»ƒè„šæœ¬
â”œâ”€â”€ test.py        (300è¡Œ) - å•å…ƒæµ‹è¯•
â”œâ”€â”€ quickstart.py  (140è¡Œ) - äº¤äº’å¼å¯åŠ¨
â””â”€â”€ pyproject.toml        - é¡¹ç›®é…ç½®
```

## ğŸ¯ æ ¸å¿ƒåŠŸèƒ½

### model.py
- **MultiHeadAttention**: 8ä¸ªå¹¶è¡Œæ³¨æ„åŠ›å¤´ï¼Œç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›
- **TransformerBlock**: è‡ªæ³¨æ„åŠ› + å‰å‘ç½‘ç»œ + æ®‹å·®è¿æ¥
- **PositionalEncoding**: æ­£å¼¦æ³¢ä½ç½®ç¼–ç 
- **NanoLLM**: å®Œæ•´æ¨¡å‹ï¼Œæ”¯æŒè‡ªå›å½’ç”Ÿæˆ

### train.py
- **TinyStoriesDataset**: è‡ªåŠ¨åŠ è½½270ä¸‡ä¸ªæ•…äº‹
- **GPT-2åˆ†è¯å™¨**: 50,257ä¸ªtokenè¯æ±‡è¡¨
- å®Œæ•´è®­ç»ƒå¾ªç¯ã€éªŒè¯å’Œæ¨¡å‹ä¿å­˜
- æ–‡æœ¬ç”Ÿæˆæ¼”ç¤º

### test.py
- 15+ä¸ªå•å…ƒæµ‹è¯•
- åˆ†è¯å™¨é›†æˆæµ‹è¯•
- æ¨¡å‹å½¢çŠ¶éªŒè¯
- æ¢¯åº¦æµæµ‹è¯•

## ğŸ’» å¿«é€Ÿç¤ºä¾‹

### åŸºæœ¬æ¨ç†
```python
import torch
from model import NanoLLM
from transformers import AutoTokenizer

model = NanoLLM(vocab_size=50257)
tokenizer = AutoTokenizer.from_pretrained("gpt2")
input_ids = tokenizer.encode("Once upon a time", return_tensors="pt")
logits = model(input_ids)
```

### æ–‡æœ¬ç”Ÿæˆ
```python
generated = model.generate(input_ids, max_length=50)
text = tokenizer.decode(generated[0].tolist())
print(text)
```

## ğŸ“Š æ¨¡å‹é…ç½®

| é¡¹ç›® | å€¼ |
|------|-----|
| è¯æ±‡è¡¨å¤§å° | 50,256 |
| æ¨¡å‹ç»´åº¦ | 256 |
| æ³¨æ„åŠ›å¤´æ•° | 8 |
| Transformerå±‚æ•° | 4 |
| æœ€å¤§åºåˆ—é•¿åº¦ | 256 |
| æ€»å‚æ•°æ•° | 1.8M |

## âš¡ æ€§èƒ½

- æ¨ç†é€Ÿåº¦: 500-1000 tokens/s (CPU)
- æ¨ç†é€Ÿåº¦: 5000+ tokens/s (GPU)
- æ˜¾å­˜å ç”¨: 100-200 MB

## ğŸ”§ ä¿®æ”¹å‚æ•°

ç¼–è¾‘ `train.py` ä¸­çš„å‚æ•°ï¼š

```python
# æ¨¡å‹å¤§å°
d_model = 256           # æ¨¡å‹ç»´åº¦
num_heads = 8           # æ³¨æ„åŠ›å¤´æ•°
num_layers = 4          # Transformerå±‚æ•°

# è®­ç»ƒé…ç½®
batch_size = 32         # æ‰¹æ¬¡å¤§å°
learning_rate = 0.001   # å­¦ä¹ ç‡
num_epochs = 3          # è®­ç»ƒè½®æ•°

# æ•°æ®é…ç½®
num_samples_train = None    # None = å…¨éƒ¨æ•°æ®
max_steps_per_epoch = None  # None = æ‰€æœ‰æ­¥éª¤
```

## ğŸ“š æ•°æ®é›†å’Œåˆ†è¯å™¨

- **TinyStories**: 270ä¸‡ä¸ªçŸ­æ•…äº‹ï¼Œä¸“ä¸ºå°æ¨¡å‹è®¾è®¡
- **GPT-2åˆ†è¯å™¨**: é«˜æ•ˆçš„å­è¯ç¼–ç ï¼Œ50Kè¯æ±‡è¡¨

## ğŸ“ å­¦ä¹ å†…å®¹

- Transformeræ¶æ„åŸç†
- å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
- PyTorchæ·±åº¦å­¦ä¹ 
- HuggingFaceç”Ÿæ€é›†æˆ
- LLMè®­ç»ƒå’Œæ¨ç†

## ğŸ“ å¸¸è§å‘½ä»¤

```bash
# è®­ç»ƒ
python train.py

# æµ‹è¯•
python test.py

# äº¤äº’èœå•
python quickstart.py

# æŸ¥çœ‹å‚æ•°æ•°
python -c "from model import NanoLLM; m = NanoLLM(50257); print(sum(p.numel() for p in m.parameters()))"

# æ£€æŸ¥GPU
python -c "import torch; print('CUDA:', torch.cuda.is_available())"
```

## âœ¨ é¡¹ç›®äº®ç‚¹

âœ… ç®€æ´ï¼šåªæœ‰5ä¸ªæ–‡ä»¶
âœ… å®Œæ•´ï¼šä»æ•°æ®åˆ°æ¨¡å‹åˆ°è®­ç»ƒ
âœ… å®ç”¨ï¼šä½¿ç”¨çœŸå®æ•°æ®é›†
âœ… å¯å­¦ï¼šè¯¦ç»†ä»£ç æ³¨é‡Š
âœ… å¯æµ‹ï¼šå…¨é¢å•å…ƒæµ‹è¯•

---

**ç«‹å³å¼€å§‹**ï¼š`python train.py`

